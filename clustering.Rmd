---
title: "Measuring similarities between polish economy and politics across years. Clustering and dimensionality reduction for International Political Economy dataset."
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: yes
      smooth_scroll: no
author: "Mateusz Bary≈Ça"
date: "05 02 2021"
bibliography: bibliography.bibtex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      results = 'asis')
pacman::p_load(tidyverse, kableExtra, tidyverse, cluster, factoextra, dendextend,
               summarytools, stats, flexclust, fpc, clustertend, ClusterR, clValid,
               mclust, kohonen)
options(warn=-1)
st_options(plain.ascii = FALSE,
           style        = "rmarkdown",
           footnote     = NA,
           subtitle.emphasis = FALSE)
```

# Introduction

Definition denotes International Political Economy as field 
that focus on a particular range of questions and a series of assumptions 
about the nature of the international system and how it is understood. 
Critical part of it concentrate on a set of questions.
Four main aspects of it concentrate on: politics, economics, 
international politics and international economics. 
International Political Economy is apparently linked to foreign policy
that impact of IPE is difficult to assess [@10.2307/2623442].

# Dataset preprocessing

## The nature of the dataset

In order to provide master dataset [@DVN/X093TV_2016] that can be used by researchers from
international political economy 89 data resources have been merged. Observations
are identified by country-year the unit of analysis. Countries are identified by 
Gleditsch-Ward number in an alternative version by Correlates of War (COW).
Most of the dataset components begin after Second World War. 
Each of 89 dataset has been given a unique suffix that uniquely identifies it.

## Preprocessing

Firstly data is loaded, then it is converted to tibble as tidyverse will be 
mainly used in this project.

```{r}
load(paste0("data/", "master_ipe_v4.rdata"))
ipe_mapping <- xlsx::read.xlsx2("data\\ipe_mapping.xlsx", sheetIndex = 1)
ipe_v4 <- as_tibble(ipe_v4)
```

Dimensions of the dataset needs to be investigated as well.

```{r}
ipe_v4 %>% dim()
```

Master table consists of 25850 rows and 1043 variables. Each of them is described 
in the codebook provided by library maintainers. They are divided into following 
groups:

* economic
* political
* social and cultural
* geographic 
* other (e.g. infrastructure, military)

Very good quality of the data is provided by Varieties of Democracy Dataset (VDEM)
inside political section, it will definitely be included not only because of 
it but also its popularity and recommendation during classes.

The other dataset chosen by me is going to be "Polity IV Democracy".

```{r}
pol_ds <- 
  ipe_v4 %>% 
  filter(country == "Poland", year > 1918, 
         !(year %in% c(1939, 1940, 1941, 1942, 1943, 1944, 1945, 2020))) %>% 
  select(year, starts_with("v2"), contains("P4"), 
         -c(change_P4, sf_P4, fragment_P4, regtrans_P4, countryname_raw_P4, 
            v2x_hosinter_VDEM, v2x_suffr_VDEM, v2elsrgel_VDEM, v2elreggov_VDEM,
            v2xlg_leginter_VDEM, v2x_elecreg_VDEM, v2xlg_elecreg_VDEM)) 
```


In every modeling process we should perform exploratory data analysis before moving to 
the next points. Thanks to the broad landscape of autoEDA packages [@Staniak_2019] we can very easily generate 
some intuition towards the analyzed data. The chosen subset of data is associated 
with brilliant starting point for analysis having only two columns,
v2x_gender_VDEM and v2x_genpp_VDEM, with missing value. The graphs show distribution
for each variable. The year column shows that there is no data between years 1939-1948.
Most of the variables there are so called indexes with range between 0 and 1.
For P4 varialbes the minimum value can be lower than zero. Durable_P4 variable is 
highly skewed. For v2xcl_rol_VDEM we can clearly see opposite pattern, however 
there is a missing range of numbers for which we have little to no observations. 

```{r}
dfSummary(pol_ds, plain.ascii = FALSE, style = "grid", 
          graph.magnif = 0.75, valid.col = FALSE, tmp.img.dir = "/tmp")
```

# Clustering

Clustering in an unsupervised learning technique that uses machine learning 
algorithms to assign similar data to groups. Mostly it is used for knowledge 
discovery of hidden patterns found within data. The main idea is to cluster 
similar observations so that they are grouped together. The more diverse groups
the better. In an ideal world, the results obtained from clustering should not 
only have good statistical properties (compact, well-separated, connected and 
stable), but also yield output that is relevant in terms of processed data.

## Clusterability of the dataset

There is a need to scale data before a further analysis. What is more NAs should 
be filled with chosen technique. 

```{r}
pol_ds_scaled <- pol_ds %>% fill(v2x_gender_VDEM, v2x_genpp_VDEM) %>% scale %>% as_tibble()
```

First step that this analysis will cover is the clusterability of the dataset.
Agnes function provides the data scientist with the agglomerative coefficient 
that measures the amount of clustering structure found plus easy and novel way 
to visualize the banner. Coefficient that is close to one suggest the high 
clustering structure found in the dataset.

```{r}
hc2 <- agnes(pol_ds_scaled)
print(hc2$ac)
```

The value suggest that chosen dataset is a very good starting point for the 
clustering analysis.

```{r}
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")
ac <- function(x) {
  agnes(pol_ds_scaled, method = x)$ac
}
map_dbl(m, ac)
```

Other methods also yield very promising results. 

## Optimal number of clusters

In order to keep high heterogeneity between the clusters and low inside them 
appropriate number of clusters should be chosen. Very simple method is defined 
as square root of (n/2) where n is denoted as optimal number of clusters.

```{r}
sqrt((pol_ds_scaled %>% nrow()) / 2)
```

Now we are coming back to elbow method.

```{r}
opt <- Optimal_Clusters_KMeans(pol_ds_scaled, max_clusters=10, plot_clusters = TRUE)
```

Elbow method and showed within-group heterogeneity shows that 6 clusters is
appropriate number of clusters, others may argue that 3 is the starting 
point of the linear trend decrease.

```{r}
opt <- Optimal_Clusters_KMeans(pol_ds_scaled, 
                               max_clusters=10, 
                               'euclidean', 
                               plot_clusters = TRUE, 
                               criterion = 'silhouette')
```

The other method uses silhouette index to find best number of groups inside the data set which 
shows the maximum value for 2 clusters and lowest for 6. Measure method takes 
into account what is the difference between points that belong to the same 
cluster and what is the difference between different clusters. 

Methods presented above are not very formalized due to their graphical way 
of presenting results. One way to address this drawback is a statistical 
procedure called *gap statistics* [@gap_Statistics]. The concept is based on the 
graph of log(W_k) by making comparison with its expectation under an appropriate
null reference distribution of the data. For latter one appropriate reference distribution 
need to be found in the data. Then the estimation of the optimal number of 
clusters is the value of k for which log(W_k) falls the farthest below this 
reference curve. One of the assumption of this algorithm is to have data 
that can be separated well.

```{r, echo = FALSE}
kable_head <- function(df){
  df %>% kable() %>% head()
}
```


```{r}
clusGapRes <- clusGap(pol_ds_scaled, FUN = kmeans, nstart = 20, K.max = 20, B = 60)
```

Number of clusters proposed by this sophisticated method is equal to 13 which is 
presented by *"Number of clusters (method 'firstSEmax', SE.factor=1): 13"*. Mainly 
due to its advanced methodology this number has been chosen in further analysis.

```{r}
chosen_size <- 13
```

Clustering based on partition assumes that cluster center is to regard the 
center of data points. Main pros of these kind of algorithms are their time 
complexity and their ability to compute results very well in general. In terms of 
drawbacks we can list following points: 

* do not work well with convex data,
* are sensitive to the outliers, 
* can be stuck in the local optimal,  
* the number of clusters before the process must be specified
* it is essentially very sensitive to the number of clusters.

## K-means

### Definition of K-means

K-means is probably the most widely used algorithm for unsupervised learning.
In the algorithm we shall specify number of clusters. A goal is pretty 
straightforward, to minimize the differences within cluster and maximize 
the differences between clusters. The way algorithm works is as follows, each of
the points is assigned to the cluster, then it updates the assignments by adjusting 
cluster boundaries according to the examples that currently fall into the cluster.
Points are assigned to the clusters based on chosen distance metric. Then 
centers are recalculated. The process lasts until no further improvements can 
be made in the process. Improvement to this algorithm is K-medoids which also
deal with discrete data points, as the representative of the corresponding cluster.

### Clustering

```{r}
pol_clusters <- kmeans(pol_ds_scaled %>% as.matrix(), chosen_size)
pol_ds$kmeans <- pol_clusters$cluster
```

```{r}
fviz_cluster(pol_clusters, data = pol_ds_scaled)
```

### Evaluation indicators

One of the metric for k-means clustering can be checking what is the ratio between cluster sum of squares and total sum of squares:

```{r}
(pol_clusters$betweenss /pol_clusters$totss)*100
```

94.50958 is the total variance in the data that is explained by the clustering.
Thanks to clustering the algorithm is able to reduce sum of squares to 94.5%.

```{r}
sil <- silhouette(x = pol_ds$kmeans, dist(pol_ds_scaled))
fviz_silhouette(sil)
```

## PAM

### Definition of PAM

PAM focuses on object that are centrally located in a cluster. The procedure 
chooses set of medoids and iteratively replace one of the medoids with one 
of the non-medoids and check if it improves the total distance of the 
resulting clustering. The algorithm stops when there is no further change.

### Clustering

```{r}
c1<-pam(pol_ds_scaled, chosen_size)
pol_ds$pam <- c1$clustering
```

```{r}
fviz_cluster(c1, geom="point", ellipse.type="norm") 
```


## CLARA

### Definition of CLARA

CLARA draws multiple samples of the dataset and then applies PAM on each sample. Therefore the main weakness of the PAM procedure is omitted.

### Clustering

```{r}
clara_flex <- eclust(pol_ds_scaled %>% as.matrix(), "clara", k=chosen_size) 
pol_ds$clara <- clara_flex$clustering
```

After assigning we can easily show counts per cluster. 

```{r}
fviz_cluster(clara_flex, geom="point", ellipse.type="norm") 
```

# Hierarchical clustering

Main advantage of hierarchical clustering over k-means is that there is no need
to implicitly specify number of clusters. The algorithm itself proceeds iteratively,
until there is just a single cluster. The algorithm is very often visualized
via dendogram which is a tree structure. Two main types of hierarchical 
clustering are widely discussed which are agglomerative [bottom up] 
and divisive [top down]. These algorithms work well with arbitrary shape and 
types. The algorithms are often able to detect hierarchical relationships which 
can be useful in many disciplines such as biology or economics. Unfortunately,
these types of algorithms do not work efficiently with large data sets.

## Agglomerative Nesting

```{r}
hc3 <- agnes(pol_ds_scaled, method = "ward")
pltree(hc3, cex = 0.6, hang = -1, main = "dendrogram - agnes")
```

In order to perform hierarchical clustering euclidean distance is calculated first
then we cut the dendogram tree at k = 6. The chosen method is Ward's method 
which is usually fine by default.

```{r}
d_pol_ds <- dist(pol_ds_scaled, method = "euclidean")
hc5 <- hclust(d_pol_ds, method = "ward.D2" )
sub_grp <- cutree(hc5, k = chosen_size)

pol_ds <- 
  pol_ds %>%
  mutate(hier_ward_D2 = sub_grp) 

```


```{r}
fviz_cluster(list(data = pol_ds_scaled, cluster = pol_ds$hier_ward_D2))
```


# Some of the other clustering methods

## Divisive clustering

```{r}
hc4 <- diana(pol_ds_scaled)
pltree(hc4, cex = 0.6, hang = -1, main = "dendrogram - diana")
sub_grp <- cutree(hc4, k = chosen_size)

pol_ds <- 
  pol_ds %>%
  mutate(hier_diana = sub_grp) 

```

In order to perform hierarchical clustering euclidean distance is calculated first
then we cut the dendogram tree at k = 6. The chosen method is Ward's method 
which is usually fine by default. 

## Evaluation indicators

Having discussed mostly known methods other types of cluster validation can be 
discussed as well such as "internal" "stability" from clValid package [@JSSv025i04]. 
For internal measures selected measures are defined the compactness, connectedness,
and separation of the cluster partitions. Connectedness is evaluated depending 
on the number of the nearest points that are assigned to the same cluster. 
Compactness relates to inner cluster homogeneity, very often being measured by 
looking at the intra-cluster variance. Distance between cluster centroids is 
often being measured when separation evaluation is presented. The non-linear 
combination of the compactness and separation is described by the Dunn index 
and silhouette width. Stability is evaluated in terms of consistency of a 
clustering by comparing it with the clusters obtained after each 
column is removed, one at a time. 

```{r}
intern <- clValid(pol_ds_scaled %>% as.matrix(), 
                  chosen_size, 
                  clMethods = 
                    c("hierarchical", "kmeans", "diana", "agnes", "pam",
                      "clara", "model"), 
                  validation = "internal"
                  )
intern@measures %>% 
  as_tibble() %>% 
  pivot_longer(cols = everything()) %>% 
  separate(name, into = c("number", "model_name"), sep="[.]") %>% 
  select(model_name, value) %>% 
  mutate( measure_id = 0:20 %/% 7) %>% 
  inner_join(tibble(measure_id = 0:2, measure_name = c("Connectivity", "Dunn", "Silhouette"))) %>%
  select(-measure_id) %>% 
  pivot_wider(names_from = measure_name, values_from = value) %>% 
  kable_head()
```

The highest connectivity is associated with PAM, CLARA and model clustering 
algorithms. It should be minimized so taking into account this technique 
the best algorithm is diana. Silhouette value is smallest for model based 
and the maximum for diana algorithm. Based on this data we can assume diana as 
the best algorithm.

## Choosing appropriate names for clusters

Based on per cluster results we can try to name formed clusters based on these 
values in terms of international economy.

```{r, fig.height=20, fig.width=12}
pol_ds_long <- 
  pol_ds %>% 
  select(-year, -kmeans, -pam,
  -clara, -hier_ward_D2) %>% 
  select(starts_with("v2x_"), hier_diana) %>% 
  pivot_longer(cols = c(-hier_diana)) %>% 
  inner_join(ipe_mapping, by = c("name" = "variable"))
 
pol_ds_long %>% 
  ggplot(aes(x = factor(hier_diana), 
             y = value, 
             fill= factor(hier_diana))) +
  geom_boxplot() +
  facet_wrap(~pretty_name, ncol = 3, shrink = FALSE, scales = "free") +
  labs(title = 'Boxplots per variable per cluster') +
  xlab("Cluster") + 
  ylab("Value") +
  theme(legend.position = "none")
```

Describing clusters with so many variables might be complicated, the
first insight that is very easily visible is that *cluster 9* has 
the highest IQR across most of the dimensions. Clusters between 9 
and 13 are very similar to each other which might suggest that 13 
clusters might sort of overfits the data. 

```{r, fig.height=10, fig.width=12}
pol_ds_long <-
    pol_ds %>% 
    select(-year, -kmeans, -pam,
           -clara, -hier_ward_D2) %>% 
    select(-contains("v2"), hier_diana) %>% 
    pivot_longer(cols = c(-hier_diana)) %>% 
    inner_join(ipe_mapping, by = c("name" = "variable"))
 
pol_ds_long %>% 
  ggplot(aes(x = factor(hier_diana), 
             y = value, 
             fill= factor(hier_diana))) +
  geom_boxplot() +
  facet_wrap(~pretty_name, ncol = 5, shrink = FALSE, scales = "free") +
  labs(title = 'Boxplots per variable per cluster') +
  xlab("Cluster") + 
  ylab("Value") +
  theme(legend.position = "none")
```

List of years for each cluster which can be used for evaluation of clustering
quality using political economy knowledge.

```{r}
pol_ds %>% 
  group_by(hier_diana) %>% 
  mutate(year = paste0(year, collapse=",")) %>% 
  slice(1) %>% 
  select(year) %>% 
  kable_head()
```

Short description of each cluster by year and box plots variables:

*1. First cluster: observations that mostly correspond to post-war period.*
Regulation of participation are very low and since it is the heart of 
democracy, they indicates that there are rules for when and how elections are 
expressed [@10.1371/journal.pone.0045838].
*2. Second cluster: political developments in Poland, Pi≈Çsudski resigned from office, 
The May Coup resulted in overthrowing President Stanis≈Çaw Wojciechowski and
Prime Minister Wincenty Witos who were democratically elected.* What definitely 
distinguishes this cluster is elected executive index which not essentialy 
should highly correspond to decision what was the level of democracy, it can rather be used to
aggregate higher-order indices. It tries to measure whether chief executive
was elected directly or indirectly. Of course corresponding to the aforementioned 
events in Poland it is clear that this metric shall be rather low.
*3. Third cluster: before second World war, new Polish Constitution has been adopted, Pi≈Çsudski dies.* What can be not very easily spotted here is the divided party control index.  [@10.2307/419392]
*4. Fourth cluster: 7 important years for defense minister of Poland, Konstanty Rokossowski.* Women political participation index value has large interquartile range which can
arise from Stanilist employment policies that aimed at the equality of sexes [@10.2307/3185730].
*5. Fifth cluster: year that mainly has been affected in politcs by nearly 50,000 political prisoners that were being held in prison* that is also visible on boxplot 
of Physical violence index, that is very low which indicates that citizens were not really safe.
*6. Sixth cluster: one of the most heterogeneous cluster in term of years, 4 different decades falls
into this group of observations.* All dimensions despite the corruption related and regime
durability
have very low IQR.
*7. Seventh cluster: second very heterogeneous cluster,* the highest number number of observations
belongs here. Similar to cluster sixth with medians slightly below medians of cluster sixth.
*8. Eighth cluster: Round Table Talks that lead to political and economical reforms.* Since it 
is only one observation boxplots does not bring a lot of value here. It can
be described as some vertical line that divides polish history into two separate 
periods which is essentially true.
*9. Ninth cluster: in 1990 Lech Wa≈Çƒôsa won the presidential election, in 1995 
Wa≈Çƒôsa demanded Pawlak's resignation in January 1995.* Cluster that has the 
highest IQR across most of the dimensions.
*10. Tenth cluster: times after Round Table Talks excluding presidential elections.*
Deliberative democracy index has the highest value.
*11. Eleventh cluster: presidential elections years.* This is the onlu group of observations where the political competition
has different values of quartiles.
*12. Twelfth cluster: Lech Wa≈Çƒôsa and Lech Kaczy≈Ñski presidency. *We can cearly 
see that egalitarian component index shows here that the resources inside 
society has been distributed equally in society.
*13. Thirteenth cluster: Andrzej Duda presidency and PiS single-party government.*
Even though women political participation index showed very high values.


# Some of the other clustering methods

## Gaussian mixture models

The main  idea that is hidden in this algorithm is that points belong to several
Gaussian distributions. The main advantages is that there is a proper mathematical
theory developed inside which can lead to very realistic probability of belonging.
Considering drawbacks we cannot ommit is that many parameters need to be specified
for these algorithms.

## Density based algorithms

The main idea behind these kind of algorithms is that cluster is assigned based on points density distribution of data points. DBSCAN algorithm is the most widely discussed algorithm that takes into account two parameters. The first one is the radius of the neighborhood. The second one is is the minimum number of points that belongs to the neighborhood.

## Model Based Clustering

The most important part of this algorithms is that for each cluster different, the best model is fitted. Two main kinds of model-based clustering algorithms can be distinguished which are based on statistical learning method (COBWEB, GMM) and the other based on neural network learning method (SOM and ART).

# Dimensionality Reduction

The idea that is behind dimensionality reduction is to present dataset 
by fewer dimensions in order to make it more interpretable, easier to visualize,
less computationally heavy. Having a dataset with a lot of variables they 
might be correlated with each other and a variable can be presented as 
a combination of other features. What is important new features should not
be correlated between themselves! 

The most common approach for reducing number of features is the following:
1. Decide on data standardization. 
2. Compute covariance matrix with all possible pairs.
3. Based on point 2 result compute eigenvectors and eigenvalues for PCA.
4. Find top eigenvectors and eigenvalues.
5. New principal components are variables as constraint of the variables with highest variance.
6. Develop new features with columns as eigenvectors.
7. Recast data along new principal axes.

```{r}
pacman::p_load(corrplot, maptools, smacof, gridExtra, psych)
```

## Checking correlations
```{r}
pol_ds_scaled_cor <- cor(pol_ds_scaled, method="pearson")
corrplot(pol_ds_scaled_cor, order ="alphabet", tl.cex=0.6)
```

Although there a lot of variables on the plot we can clearly see that a lot of variables are correlated either negatively or positively. 

## MDS reduction

As our data relates to social sciences we can definitely start with multidimensional scaling. Starting with definition, this is a nonlinear statistical technique originated in psychometrics. MDS keeps the distance between the points and reduces number of dimensions. Distances allow to preserve pattern and clusters. The metrics used in this methods, called proximity, describes there is a number that indicates how similar two objects are or are perceived to be. 

## Classification of MDS

In order to describe scaling models they can be broadly classified into metric vs non-metric and strain (classical scaling) vs stress (distance scaling) based MDS models. 

## Applications 

As MDS is one of the most widely used MDS techniques in data analysis of course it is not possible to provide a complete set of its uses. They find several applications in image processing or pre-process for algorithms that rely on Euclidean distances.  

```{r}
dist_pol_ds_scaled<-dist(t(pol_ds_scaled)) # as input we need distance between units
mds3<-cmdscale(dist_pol_ds_scaled, k=2) #k - the maximum dimension of the space
plot(mds3, type='n') # plot with labels
pointLabel(mds3, labels = pol_ds %>% mutate(year = as.character(year)) %>% pull(year), cex=0.6, adj=0.5)
```

Using smacof package allows for different measurement scales of data such as
ratio, interval, oridinal and mspline. 

```{r}
dis2<-sim2diss(pol_ds_scaled_cor, method=1, to.dist=TRUE)
fit.data<-mds(dis2, ndim=2,  type="ratio") 
plot(fit.data, pch=21, cex=as.numeric(fit.data$spp), bg="red")
```


### Measuring goodness of MDS 

In order to measure how good is MDS functionality stress function can be 
taken into account. 

```{r}
fit.data$stress
```

Proposes values have been proposed for measuring the goodness of the fit. 0.1 which
is our case provides information that this is a fair result. This value should be 
minimized. For this different methods can be analyzed.

```{r}
dis2<-sim2diss(pol_ds_scaled_cor, method=1, to.dist=TRUE)
fit.data<-mds(dis2, ndim=2,  type="ordinal") # from smacof::
fit.data$stress
```

Smallest value is yield by ordinal method which is non-metric MDS type where only thing that is important is the order relations between the dissimilarities.

## Principal Component Analysis

```{r}
xxx.pca1<-prcomp(pol_ds_scaled, center=FALSE, scale.=FALSE) 
```

For explaining 99% of the variance we need to take only first two Principal
Components.

```{r}
xxx.pca2<-princomp(pol_ds_scaled) # stats::princomp()
plot(xxx.pca2)# the same will be plot(xxx.pca1)
fviz_pca_var(xxx.pca1, col.var="steelblue")
```

Based on this plot the following intuition can be achieved: 
parreg_P4 is oppositely correlated to the v2x_divparctrl_VDEM.

```{r}
fviz_eig(xxx.pca1, choice='eigenvalue') # eigenvalues on y-axis
fviz_eig(xxx.pca1) # percentage of explained variance on y-axis
```

```{r}
var<-get_pca_var(xxx.pca1)
a<-fviz_contrib(xxx.pca1, "var", axes=1, xtickslab.rt=90) # default angle=45√Ç¬∞
b<-fviz_contrib(xxx.pca1, "var", axes=2, xtickslab.rt=90)
grid.arrange(a,b,top='Contribution to the first two Principal Components')
```

Rotated PCA allows easier interpretation of factors used in analysis due to the change in structure of components. PCA rotates components across the axis of the factors. The most commonly used option is varimax which minimizes the number of variables needed to explain a given factors. 

```{r}
xxx.pca4<-principal(pol_ds_scaled, nfactors=2, rotate="varimax")
```

```{r}
summary(xxx.pca4)
```


```{r}
plot(xxx.pca1)
plot(xxx.pca1, type = "l")
fviz_eig(xxx.pca1)
```

## Uniqueness and complexity

Uniqueness is the relation of variance according to share in other variables. For PCA we want it to be low, then we know that the variable is not correlated with other variables in the model. This is similar to multicollinearity. 

Complexity corresponds to the number of factor loads that take values greater than zero. Given the relatively large loading of one factor only the complexity is near to 1. To paraphrase, we want to know what is the overall contribution of all factors to the single variable. We want to keep our contribution low because it involves a more difficult interpretation of factors.

```{r}
set<-data.frame(complex=xxx.pca4$complexity, unique=xxx.pca4$uniqueness)
set.worst<-set[set$complex>1.8 & set$unique>0.78,]
set.worst %>% kable_head
```

Combining complexity and uniqueness together we can spot variables that may lead to problems. 

```{r}
fviz_pca_ind(xxx.pca1, col.ind="cos2", geom="point", gradient.cols=c("white", "#2E9FDF", "#FC4E07" ))
```

# Summary

The problem of estimating number of clusters in dataset can be quite hard 
which then result in yielding unexpected results. Domain experts 
may claim that 13 clusters in dataset with 89 observations is surprisingly a lot.
On the other hand the so called gap statistics have been used along with
cluster validation package that can help with task that are performed 
without external knowledge. 

The data set collections itself was very useful in understanding problems 
of International Political Economy. The results of clustering are very promising
in terms of their proposed definition that suggests that polish history 
can be very appealing taking into account its variability.

# Bibliography
