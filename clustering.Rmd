---
title: "UL project clustering and dimension reduction"
author: "Mateusz Bary≈Ça"
date: "6 12 2020"
output: html_document
bibliography: bibliography.bibtex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      results = 'asis')
pacman::p_load(tidyverse, kableExtra, tidyverse, cluster, factoextra, dendextend,
               summarytools, stats, flexclust, fpc, clustertend, ClusterR)

st_options(plain.ascii = FALSE,        # Always use this option in Rmd documents
           style        = "rmarkdown", # Always use this option in Rmd documents
           footnote     = NA,          # Makes html-rendered results more concise
           subtitle.emphasis = FALSE)  # Improves layout with some rmardown themes
```

## Introduction

Clustering in an unsupervised learning technique that uses machine learning 
algorithms to assign similar data to groups. Mostly it is used for knowledge 
discovery of hidden patterns found within data. The main idea is to cluster 
similar observations that they are grouped together. The more diverse groups
the better.

## Dataset

### International Politcal Economy

International Political Economy is a discipline that discusses economics, politics
and international relations. The main areas of economics that are covered by IPE 
are the following fields of macroeconomics, international business, international 
development and development economics. 

### The nature of the dataset

In order to provide master dataset that can be used by researchers from
international political economy 89 data resources have been merged. Observations
are identified by country-year the unit of analysis. Countries are identified by 
Gleditsch-Ward number ot in an alternative version by Correlates of War (COW).
Most of the dataset components begin after Second World War. 

### Variable naming

Each of 89 dataset has been given a unique suffix that uniquely identifies it.

## Dataset preprocessing

Firstly data is loaded, then we convert it to tibble as tidyverse will be 
mainly used in this project.

```{r}
load(paste0("data/", "master_ipe_v4.rdata"))
ipe_v4 <- as_tibble(ipe_v4)
```

Dimensions of the dataset needs to be investigated as well.

```{r}
ipe_v4 %>% dim()
```


Master table consists of 25850 rows and 1043 variables. Each of them is described 
in the codebook provided by library maintainers. They are divided into following 
groups:
* economic
* political
* social and cultural
* geographic 
* other (e.g. infrastructure, military)

Very good quality of the data is provided by Varities of Democracy Dataset (VDEM)
inside political section, it will definitely be included not only because of 
the quality but also its popularity and inside class recommendation. 

The other  dataset chosen by me is going to be "Polity IV Democracy".

```{r}
pol_ds <- 
  ipe_v4 %>% 
  filter(country == "Poland", year > 1918, 
         !(year %in% c(1939, 1940, 1941, 1942, 1943, 1944, 1945, 2020))) %>% 
  select(year, starts_with("v2"), contains("P4"), 
         -c(change_P4, sf_P4, fragment_P4, regtrans_P4, countryname_raw_P4, 
            v2x_hosinter_VDEM, v2x_suffr_VDEM, v2elsrgel_VDEM, v2elreggov_VDEM,
            v2xlg_leginter_VDEM, v2x_elecreg_VDEM, v2xlg_elecreg_VDEM)) 
pol_ds %>%
  head() %>% 
  kable()
```

## Exploratory data analysis

In every modeling process we should perform exploratory data analysis before moving to 
the next points. 

```{r}
dfSummary(pol_ds, plain.ascii = FALSE, style = "grid", 
          graph.magnif = 0.75, valid.col = FALSE, tmp.img.dir = "/tmp")
```

Thanks to the broad landscape of autoEDA packages we can very easily generate 
some intuition towards the analyzed data. The chosen subset of data is associated 
with brilliant starting point for analysis having only two columns,
v2x_gender_VDEM and v2x_genpp_VDEM, with missing value. The graphs show distribution
for each variable. The year column shows that there is no data between years 1939-1948.
Most of the variables there are so called indexes with range between 0 and 1.
For P4 varialbes the minimum value can be lower than zero. Durable_P4 variable is 
highly skewed. For v2xcl_rol_VDEM we can clearly see oposite pattern, however  
there is a misssing range of numbers for which we have little to no observations. 

## Clustering

### Clusterability of the dataset

There is a need to scale data before an further analysis and fill NAs with chosen 
technique. 

```{r}
pol_ds_scaled <- pol_ds %>% fill(v2x_gender_VDEM, v2x_genpp_VDEM) %>% scale %>% as_tibble()
```

# Test for clusterability of data

First step that this analysis will cover is the clusterability of the dataset.
Agnes function provides the data scientist with the agglomerative coefficient 
that measures the amount of clustering structure found plus easy and novel way 
to visualize the banner. Coefficient that is close to one suggest the high 
clustering structure found in the dataset.

```{r}
hc2 <- agnes(pol_ds_scaled)
print(hc2$ac)
```

The value suggest that chosen dataset is a very good starting point for the 
clustering analysis.

```{r}
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")
ac <- function(x) {
  agnes(pol_ds_scaled, method = x)$ac
}
map_dbl(m, ac)
```

Other methods also yield very good results. 

## Optimal numebr of clusters
In order to keep high heterogeneity between the clusters and low inside them 
appropriate number of clusters should be chosen. Very simple method is defined 
as square root of (n/2) where n is denoted as optimal number of clusters.

```{r}
sqrt((pol_ds_scaled %>% nrow()) / 2)
```
Nowe we are coming back to elbow method in order to investigate optimal 
number of clusters.

```{r}
opt <- Optimal_Clusters_KMeans(pol_ds_scaled, max_clusters=10, plot_clusters = TRUE)
```

Elbow method and showed within-group heterogeneity shows that 6 clusters is
appropriate number of clusters.

```{r}
opt <- Optimal_Clusters_KMeans(pol_ds_scaled, max_clusters=10, 'euclidean', 
                               plot_clusters = TRUE, criterion = 'silhouette')
```

The other method uses silhouette index to find optimal number of clusters which 
shows the maximum value for 2 clusters and lowest for 6.

## K-means

Kmeans is probably the most widely used algorithm for unsupervised learning.
In the algorithm we shall specify number of clusters. A goal is pretty 
straightforward, to minimize the differences within cluster and maximize 
the differences between clusters. The way algorithm works is as follows, each of
the points is assigned to the cluster, then it updates the assignments by adjusting 
cluster boundaries according to the examples that currently fall into the cluster.
Points are assigned to the clusters based on chosen distance metric. Then 
centers are recalculated. The process lasts until no further improvements can 
be made in the process.

```{r}
pol_clusters <- kmeans(pol_ds_scaled %>% as.matrix(), 6)
pol_ds_scaled$cluster <- pol_clusters$cluster
```

After assigning we can easily show counts per cluster. 

```{r}
pol_ds_scaled %>% group_by(cluster) %>% count()
```

One of the metric for kmeans clustering can be checking what is the ratio 
between cluster sum of squares and total sum of squares:

```{r}
(pol_clusters$betweenss /pol_clusters$totss)*100
```

The 87.46361 is the total variance in the data that is explained by the clustering.
Thanks to clustering the algorithm is able to reduce sum of squares to 87.4631%.

```{r}
sil <- silhouette(x = pol_ds_scaled$cluster, dist(pol_ds_scaled))
fviz_silhouette(sil)
```

## Hierarchical clustering

Main advantage of hierarchical clustering over k-means is that there is no need
to implicitly specify number of clusters. The algorithm itself proceeds iteratively,
until there is just a single cluster. The algorithm is very often visualized
via dendogram which is a tree structure. Two main types of hierarchial 
clustering are widely discussed which are agglomerative [bottom up] 
and divisive [top down]. 

```{r}
hc3 <- agnes(pol_ds_scaled, method = "ward")
pltree(hc3, cex = 0.6, hang = -1, main = "dendrogram - agnes") 
hc4 <- diana(pol_ds_scaled)
pltree(hc4, cex = 0.6, hang = -1, main = "dendrogram - diana") 
```

In order to perform hierarchical clustering euclidean distance is calculated first
then we cut the dendogram tree at k = 6. The chosen method is Ward's method 
which is usually fine by default. 

```{r}
d_pol_ds <- dist(pol_ds_scaled, method = "euclidean")
hc5 <- hclust(d_pol_ds, method = "ward.D2" )
sub_grp <- cutree(hc5, k = 6)

pol_ds_scaled %>%
  mutate(cluster = sub_grp) %>%
  group_by(cluster) %>% 
  count()
```


