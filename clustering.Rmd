---
title: "UL project clustering and dimension reduction"
author: "Mateusz Bary≈Ça"
date: "6 12 2020"
output: html_document
bibliography: bibliography.bibtex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      results = 'asis')
pacman::p_load(tidyverse, kableExtra, tidyverse, cluster, factoextra, dendextend,
               summarytools, stats, flexclust, fpc, clustertend, ClusterR, clValid)

st_options(plain.ascii = FALSE,        # Always use this option in Rmd documents
           style        = "rmarkdown", # Always use this option in Rmd documents
           footnote     = NA,          # Makes html-rendered results more concise
           subtitle.emphasis = FALSE)  # Improves layout with some rmardown themes
```

## Introduction

Clustering in an unsupervised learning technique that uses machine learning 
algorithms to assign similar data to groups. Mostly it is used for knowledge 
discovery of hidden patterns found within data. The main idea is to cluster 
similar observations that they are grouped together. The more diverse groups
the better. In an ideal world, the results obtained from clustering should not only have good statistical properties (compact, well-separated, connected and stable), but also yield output that is relevant in terms of processed data.

## Dataset

### International Politcal Economy

International Political Economy is a discipline that discusses economics, politics
and international relations. The main areas of economics that are covered by IPE 
are the following fields of macroeconomics, international business, international 
development and development economics. 

### The nature of the dataset

In order to provide master dataset that can be used by researchers from
international political economy 89 data resources have been merged. Observations
are identified by country-year the unit of analysis. Countries are identified by 
Gleditsch-Ward number ot in an alternative version by Correlates of War (COW).
Most of the dataset components begin after Second World War. 

### Variable naming

Each of 89 dataset has been given a unique suffix that uniquely identifies it.

## Dataset preprocessing

Firstly data is loaded, then we convert it to tibble as tidyverse will be 
mainly used in this project.

```{r}
load(paste0("data/", "master_ipe_v4.rdata"))
ipe_v4 <- as_tibble(ipe_v4)
```

Dimensions of the dataset needs to be investigated as well.

```{r}
ipe_v4 %>% dim()
```


Master table consists of 25850 rows and 1043 variables. Each of them is described 
in the codebook provided by library maintainers. They are divided into following 
groups:
* economic
* political
* social and cultural
* geographic 
* other (e.g. infrastructure, military)

Very good quality of the data is provided by Varities of Democracy Dataset (VDEM)
inside political section, it will definitely be included not only because of 
the quality but also its popularity and inside class recommendation. 

The other  dataset chosen by me is going to be "Polity IV Democracy".

```{r}
pol_ds <- 
  ipe_v4 %>% 
  filter(country == "Poland", year > 1918, 
         !(year %in% c(1939, 1940, 1941, 1942, 1943, 1944, 1945, 2020))) %>% 
  select(year, starts_with("v2"), contains("P4"), 
         -c(change_P4, sf_P4, fragment_P4, regtrans_P4, countryname_raw_P4, 
            v2x_hosinter_VDEM, v2x_suffr_VDEM, v2elsrgel_VDEM, v2elreggov_VDEM,
            v2xlg_leginter_VDEM, v2x_elecreg_VDEM, v2xlg_elecreg_VDEM)) 
pol_ds %>%
  head() %>% 
  kable()
```

## Exploratory data analysis

In every modeling process we should perform exploratory data analysis before moving to 
the next points. 

```{r}
dfSummary(pol_ds, plain.ascii = FALSE, style = "grid", 
          graph.magnif = 0.75, valid.col = FALSE, tmp.img.dir = "/tmp")
```

Thanks to the broad landscape of autoEDA packages we can very easily generate 
some intuition towards the analyzed data. The chosen subset of data is associated 
with brilliant starting point for analysis having only two columns,
v2x_gender_VDEM and v2x_genpp_VDEM, with missing value. The graphs show distribution
for each variable. The year column shows that there is no data between years 1939-1948.
Most of the variables there are so called indexes with range between 0 and 1.
For P4 varialbes the minimum value can be lower than zero. Durable_P4 variable is 
highly skewed. For v2xcl_rol_VDEM we can clearly see oposite pattern, however  
there is a misssing range of numbers for which we have little to no observations. 

## Clustering

### Clusterability of the dataset

There is a need to scale data before an further analysis and fill NAs with chosen 
technique. 

```{r}
pol_ds_scaled <- pol_ds %>% fill(v2x_gender_VDEM, v2x_genpp_VDEM) %>% scale %>% as_tibble()
```

# Test for clusterability of data

First step that this analysis will cover is the clusterability of the dataset.
Agnes function provides the data scientist with the agglomerative coefficient 
that measures the amount of clustering structure found plus easy and novel way 
to visualize the banner. Coefficient that is close to one suggest the high 
clustering structure found in the dataset.

```{r}
hc2 <- agnes(pol_ds_scaled)
print(hc2$ac)
```

The value suggest that chosen dataset is a very good starting point for the 
clustering analysis.

```{r}
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")
ac <- function(x) {
  agnes(pol_ds_scaled, method = x)$ac
}
map_dbl(m, ac)
```

Other methods also yield very good results. 

## Optimal number of clusters

In order to keep high heterogeneity between the clusters and low inside them 
appropriate number of clusters should be chosen. Very simple method is defined 
as square root of (n/2) where n is denoted as optimal number of clusters.

```{r}
sqrt((pol_ds_scaled %>% nrow()) / 2)
```
Now we are coming back to elbow method in order to investigate optimal 
number of clusters.

```{r}
opt <- Optimal_Clusters_KMeans(pol_ds_scaled, max_clusters=10, plot_clusters = TRUE)
```

Elbow method and showed within-group heterogeneity shows that 6 clusters is
appropriate number of clusters.

```{r}
opt <- Optimal_Clusters_KMeans(pol_ds_scaled, max_clusters=10, 'euclidean', plot_clusters = TRUE, criterion = 'silhouette')
```

The other method uses silhouette index to find optimal number of clusters which 
shows the maximum value for 2 clusters and lowest for 6. Measure method takes 
into account what is the difference between points that belong to the same 
cluster and what is the difference between different clusters. 

Methods presented above are not very formalized due to their graphical way of presenting results. One way to address this drawback is a statistical procedure called 'gap' statistics. The concept is based on the graph of log(W_k) by making comparison with its expectation under an appropriate null reference distribution of the data. For latter one appropriate reference distribution need to be found in the data. Then the estimation of the optimal number of clusters is the value of k for which log(W_k) falls the farthest below this reference curve. One of the assumption of this algorithm is to have data that can be separated well.

```{r}
clusGap(pol_ds_scaled, FUN = kmeans, nstart = 20, K.max = 20, B = 60)

```

Number of clusters proposed by this sophisticated method is equal to 13. Mainly due to its advanced methodology this number has been chosen in further analysis.

```{r}
chosen_size <- 13
```

# Typical clustering algorithms based on partition

Clustering based on partition assumes that cluster center is to regard the 
center of data points. Main pros of these kind of algorithms are their time complexity,
and their ability to compute results very well in general. In terms of drawbacks 
we can list following points: these algorithms do not work well with convex data,
they are sort of sensitive to the outliers, they can be stuck in the local 
optimal, what is more we must specify the number of clusters before the process
which is essentially very sensitive to the number of clusters.

## K-means

### Definition of K-means

K-means is probably the most widely used algorithm for unsupervised learning.
In the algorithm we shall specify number of clusters. A goal is pretty 
straightforward, to minimize the differences within cluster and maximize 
the differences between clusters. The way algorithm works is as follows, each of
the points is assigned to the cluster, then it updates the assignments by adjusting 
cluster boundaries according to the examples that currently fall into the cluster.
Points are assigned to the clusters based on chosen distance metric. Then 
centers are recalculated. The process lasts until no further improvements can 
be made in the process. Improvement to this algorithm is K-medoids which also
deal with discrete data points, as the representative of the corresponding cluster.

### Clustering

```{r}
pol_clusters <- kmeans(pol_ds_scaled %>% as.matrix(), chosen_size)
pol_ds$cluster_kmeans <- pol_clusters$cluster
```

After assigning we can easily show counts per cluster. 

```{r}
pol_ds %>% group_by(cluster_kmeans) %>% count()
```

### Evaluation indicators

One of the metric for k-means clustering can be checking what is the ratio between cluster sum of squares and total sum of squares:

```{r}
(pol_clusters$betweenss /pol_clusters$totss)*100
```

The 87.46361 is the total variance in the data that is explained by the clustering.
Thanks to clustering the algorithm is able to reduce sum of squares to 87.4631%.

```{r}
sil <- silhouette(x = pol_ds$cluster_kmeans, dist(pol_ds_scaled))
fviz_silhouette(sil)
```

Having discussed mostly known methods other types of cluster validation can be discussed as well such as "internal" "stability" from clValid package. For internal measures selected measures are defined the compactness, connectedness,
and separation of the cluster partitions. Connectedness is evaluated depending on the number of the nearest points that are assigned to the same cluster. Compactness relates to inner cluster homogeneity, very often being measured by looking at the intra-cluster variance. Distance between cluster centroids is often being measured when separation evaluation is presented. The non-linear combination of the compactness and separation is described by the Dunn index and silhouette width. Stability is evaluated in terms of consistency of a clustering by comparing it with the clusters obtained after each column is removed, one at a time. 

## PAM

### Definition of PAM

PAM focuses on object that are centrally located in a cluster. The procedure chooses set of medoids and iteratively replace one of the medoids with one of the non-medoids and check if it improves the total distance of the resulting clustering. The algorithm stops when there is no further change.

### Clustering

```{r}
c1<-pam(pol_ds_scaled, chosen_size)
pol_ds$cluster_pam <- c1$clustering
```

After assigning we can easily show counts per cluster. 

```{r}
pol_ds %>% group_by(cluster_pam) %>% count()
```

### Evaluation indicators

## CLARA

### Definition of CLARA

CLARA draws multiple samples of the dataset and then applies PAM on each sample. Therefore the main weakness of the PAM procedure is omitted.

### Clustering

```{r}
clara_flex <- eclust(pol_ds_scaled %>% as.matrix(), "clara", k=chosen_size) 
pol_ds$cluster_clara <- clara_flex$clustering
```

After assigning we can easily show counts per cluster. 

```{r}
pol_ds %>% group_by(cluster_clara) %>% count()
```

### Evaluation indicators

```{r}
fviz_cluster(clara_flex, geom="point", ellipse.type="norm") 
```

# Hierarchical clustering

Main advantage of hierarchical clustering over k-means is that there is no need
to implicitly specify number of clusters. The algorithm itself proceeds iteratively,
until there is just a single cluster. The algorithm is very often visualized
via dendogram which is a tree structure. Two main types of hierarchical 
clustering are widely discussed which are agglomerative [bottom up] 
and divisive [top down]. These algorithms work well with arbitrary shape and 
types. The algorithms are often able to detect hierarchical relationships which 
can be useful in many disciplines such as biology or economics. Unfortunately,
these types of algorithms do not work efficiently with large data sets.

## Agglomerative Nesting

```{r}
hc3 <- agnes(pol_ds_scaled, method = "ward")
pltree(hc3, cex = 0.6, hang = -1, main = "dendrogram - agnes")
```

In order to perform hierarchical clustering euclidean distance is calculated first
then we cut the dendogram tree at k = 6. The chosen method is Ward's method 
which is usually fine by default.

```{r}
d_pol_ds <- dist(pol_ds_scaled, method = "euclidean")
hc5 <- hclust(d_pol_ds, method = "ward.D2" )
sub_grp <- cutree(hc5, k = 6)

pol_ds_scaled <- 
  pol_ds_scaled %>%
  mutate(cluster_hier = sub_grp) 

pol_ds_scaled %>%
  group_by(cluster_hier) %>% 
  count()
```


```{r}
fviz_cluster(list(data = pol_ds_scaled, cluster = pol_ds_scaled$cluster_hier))
```

### Evaluation indicators

# Some of other clustering methods

## Divisive clustering

```{r}
hc4 <- diana(pol_ds_scaled)
pltree(hc4, cex = 0.6, hang = -1, main = "dendrogram - diana")
```

In order to perform hierarchical clustering euclidean distance is calculated first
then we cut the dendogram tree at k = 6. The chosen method is Ward's method 
which is usually fine by default. 

```{r}
d_pol_ds <- dist(pol_ds_scaled, method = "euclidean")
hc5 <- hclust(d_pol_ds, method = "ward.D2" )
sub_grp <- cutree(hc5, k = 6)

pol_ds_scaled <- 
  pol_ds_scaled %>%
  mutate(cluster_hier = sub_grp) 

pol_ds_scaled %>%
  group_by(cluster_hier) %>% 
  count()
```


```{r}
fviz_cluster(list(data = pol_ds_scaled, cluster = pol_ds_scaled$cluster_hier))
```

### Evaluation indicators

```{r}
intern <- clValid(pol_ds_scaled %>% as.matrix(), chosen_size, clMethods = c("hierarchical",
+ "kmeans", "diana", "fanny", "som", "pam", "sota", "clara",
+ "model"), validation = "internal")
```

# Some of other clustering methods

## Gaussian mixture models

The main  idea that is hidden in this algorithm is that points belong to several
Gaussian distributions. The main advantages is that there is a proper mathematical
theory developed inside which can lead to very realistic probability of belonging.
Considering drawbacks we cannot ommit is that many parameters need to be specified
for these algorithms.

## Density based algorithms

The main idea behind these kind of algorithms is that cluster is assigned based on points density distribution of data points. DBSCAN algorithm is the most widely discussed algorithm that takes into account two parameters. The first one is the radius of the neighborhood. The second one is is the minimum number of points that belongs to the neighborhood.

## Model Based Clustering

The most important part of this algorithms is that for each cluster different, the best model is fitted. Two main kinds of model-based clustering algorithms can be distinguished which are based on statistical learning method (COBWEB, GMM) and the other based on neural network learning method (SOM and ART).

# Explaining the results of clustering

