---
title: "International Economics dataset, clustering and dimensionality reduction"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
author: "Mateusz Baryła"
date: "6 12 2020"
bibliography: bibliography.bibtex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      results = 'asis')
pacman::p_load(tidyverse, kableExtra, tidyverse, cluster, factoextra, dendextend,
               summarytools, stats, flexclust, fpc, clustertend, ClusterR, clValid,
               mclust, kohonen)

st_options(plain.ascii = FALSE,        # Always use this option in Rmd documents
           style        = "rmarkdown", # Always use this option in Rmd documents
           footnote     = NA,          # Makes html-rendered results more concise
           subtitle.emphasis = FALSE)  # Improves layout with some rmardown themes
```

## Introduction

Clustering in an unsupervised learning technique that uses machine learning 
algorithms to assign similar data to groups. Mostly it is used for knowledge 
discovery of hidden patterns found within data. The main idea is to cluster 
similar observations that they are grouped together. The more diverse groups
the better. In an ideal world, the results obtained from clustering should not only have good statistical properties (compact, well-separated, connected and stable), but also yield output that is relevant in terms of processed data.

## Dataset

### International Politcal Economy

International Political Economy is a discipline that discusses economics, politics
and international relations. The main areas of economics that are covered by IPE 
are the following fields of macroeconomics, international business, international 
development and development economics. 

### The nature of the dataset

In order to provide master dataset that can be used by researchers from
international political economy 89 data resources have been merged. Observations
are identified by country-year the unit of analysis. Countries are identified by 
Gleditsch-Ward number ot in an alternative version by Correlates of War (COW).
Most of the dataset components begin after Second World War. 

### Variable naming

Each of 89 dataset has been given a unique suffix that uniquely identifies it.

## Dataset preprocessing

Firstly data is loaded, then we convert it to tibble as tidyverse will be 
mainly used in this project.

```{r}
load(paste0("data/", "master_ipe_v4.rdata"))
ipe_v4 <- as_tibble(ipe_v4)
```

Dimensions of the dataset needs to be investigated as well.

```{r}
ipe_v4 %>% dim()
```


Master table consists of 25850 rows and 1043 variables. Each of them is described 
in the codebook provided by library maintainers. They are divided into following 
groups:
* economic
* political
* social and cultural
* geographic 
* other (e.g. infrastructure, military)

Very good quality of the data is provided by Varities of Democracy Dataset (VDEM)
inside political section, it will definitely be included not only because of 
the quality but also its popularity and inside class recommendation. 

The other  dataset chosen by me is going to be "Polity IV Democracy".

```{r}
pol_ds <- 
  ipe_v4 %>% 
  filter(country == "Poland", year > 1918, 
         !(year %in% c(1939, 1940, 1941, 1942, 1943, 1944, 1945, 2020))) %>% 
  select(year, starts_with("v2"), contains("P4"), 
         -c(change_P4, sf_P4, fragment_P4, regtrans_P4, countryname_raw_P4, 
            v2x_hosinter_VDEM, v2x_suffr_VDEM, v2elsrgel_VDEM, v2elreggov_VDEM,
            v2xlg_leginter_VDEM, v2x_elecreg_VDEM, v2xlg_elecreg_VDEM)) 
pol_ds %>%
  head() %>% 
  kable()
```

## Exploratory data analysis

In every modeling process we should perform exploratory data analysis before moving to 
the next points. 

```{r}
dfSummary(pol_ds, plain.ascii = FALSE, style = "grid", 
          graph.magnif = 0.75, valid.col = FALSE, tmp.img.dir = "/tmp")
```

Thanks to the broad landscape of autoEDA packages we can very easily generate 
some intuition towards the analyzed data. The chosen subset of data is associated 
with brilliant starting point for analysis having only two columns,
v2x_gender_VDEM and v2x_genpp_VDEM, with missing value. The graphs show distribution
for each variable. The year column shows that there is no data between years 1939-1948.
Most of the variables there are so called indexes with range between 0 and 1.
For P4 varialbes the minimum value can be lower than zero. Durable_P4 variable is 
highly skewed. For v2xcl_rol_VDEM we can clearly see oposite pattern, however  
there is a misssing range of numbers for which we have little to no observations. 

## Clustering

### Clusterability of the dataset

There is a need to scale data before an further analysis and fill NAs with chosen 
technique. 

```{r}
pol_ds_scaled <- pol_ds %>% fill(v2x_gender_VDEM, v2x_genpp_VDEM) %>% scale %>% as_tibble()
```

# Test for clusterability of data

First step that this analysis will cover is the clusterability of the dataset.
Agnes function provides the data scientist with the agglomerative coefficient 
that measures the amount of clustering structure found plus easy and novel way 
to visualize the banner. Coefficient that is close to one suggest the high 
clustering structure found in the dataset.

```{r}
hc2 <- agnes(pol_ds_scaled)
print(hc2$ac)
```

The value suggest that chosen dataset is a very good starting point for the 
clustering analysis.

```{r}
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")
ac <- function(x) {
  agnes(pol_ds_scaled, method = x)$ac
}
map_dbl(m, ac)
```

Other methods also yield very good results. 

## Optimal number of clusters

In order to keep high heterogeneity between the clusters and low inside them 
appropriate number of clusters should be chosen. Very simple method is defined 
as square root of (n/2) where n is denoted as optimal number of clusters.

```{r}
sqrt((pol_ds_scaled %>% nrow()) / 2)
```
Now we are coming back to elbow method in order to investigate optimal 
number of clusters.

```{r}
opt <- Optimal_Clusters_KMeans(pol_ds_scaled, max_clusters=10, plot_clusters = TRUE)
```

Elbow method and showed within-group heterogeneity shows that 6 clusters is
appropriate number of clusters.

```{r}
opt <- Optimal_Clusters_KMeans(pol_ds_scaled, max_clusters=10, 'euclidean', plot_clusters = TRUE, criterion = 'silhouette')
```

The other method uses silhouette index to find optimal number of clusters which 
shows the maximum value for 2 clusters and lowest for 6. Measure method takes 
into account what is the difference between points that belong to the same 
cluster and what is the difference between different clusters. 

Methods presented above are not very formalized due to their graphical way of presenting results. One way to address this drawback is a statistical procedure called 'gap' statistics. The concept is based on the graph of log(W_k) by making comparison with its expectation under an appropriate null reference distribution of the data. For latter one appropriate reference distribution need to be found in the data. Then the estimation of the optimal number of clusters is the value of k for which log(W_k) falls the farthest below this reference curve. One of the assumption of this algorithm is to have data that can be separated well.

```{r}
clusGap(pol_ds_scaled, FUN = kmeans, nstart = 20, K.max = 20, B = 60)

```

Number of clusters proposed by this sophisticated method is equal to 13. Mainly due to its advanced methodology this number has been chosen in further analysis.

```{r}
chosen_size <- 13
```

# Typical clustering algorithms based on partition

Clustering based on partition assumes that cluster center is to regard the 
center of data points. Main pros of these kind of algorithms are their time complexity,
and their ability to compute results very well in general. In terms of drawbacks 
we can list following points: these algorithms do not work well with convex data,
they are sort of sensitive to the outliers, they can be stuck in the local 
optimal, what is more we must specify the number of clusters before the process
which is essentially very sensitive to the number of clusters.

## K-means

### Definition of K-means

K-means is probably the most widely used algorithm for unsupervised learning.
In the algorithm we shall specify number of clusters. A goal is pretty 
straightforward, to minimize the differences within cluster and maximize 
the differences between clusters. The way algorithm works is as follows, each of
the points is assigned to the cluster, then it updates the assignments by adjusting 
cluster boundaries according to the examples that currently fall into the cluster.
Points are assigned to the clusters based on chosen distance metric. Then 
centers are recalculated. The process lasts until no further improvements can 
be made in the process. Improvement to this algorithm is K-medoids which also
deal with discrete data points, as the representative of the corresponding cluster.

### Clustering

```{r}
pol_clusters <- kmeans(pol_ds_scaled %>% as.matrix(), chosen_size)
pol_ds$cluster_kmeans <- pol_clusters$cluster
```

After assigning we can easily show counts per cluster. 

```{r}
pol_ds %>% group_by(cluster_kmeans) %>% count()
```

### Evaluation indicators

One of the metric for k-means clustering can be checking what is the ratio between cluster sum of squares and total sum of squares:

```{r}
(pol_clusters$betweenss /pol_clusters$totss)*100
```

The 87.46361 is the total variance in the data that is explained by the clustering.
Thanks to clustering the algorithm is able to reduce sum of squares to 87.4631%.

```{r}
sil <- silhouette(x = pol_ds$cluster_kmeans, dist(pol_ds_scaled))
fviz_silhouette(sil)
```

Having discussed mostly known methods other types of cluster validation can be discussed as well such as "internal" "stability" from clValid package. For internal measures selected measures are defined the compactness, connectedness,
and separation of the cluster partitions. Connectedness is evaluated depending on the number of the nearest points that are assigned to the same cluster. Compactness relates to inner cluster homogeneity, very often being measured by looking at the intra-cluster variance. Distance between cluster centroids is often being measured when separation evaluation is presented. The non-linear combination of the compactness and separation is described by the Dunn index and silhouette width. Stability is evaluated in terms of consistency of a clustering by comparing it with the clusters obtained after each column is removed, one at a time. 

## PAM

### Definition of PAM

PAM focuses on object that are centrally located in a cluster. The procedure chooses set of medoids and iteratively replace one of the medoids with one of the non-medoids and check if it improves the total distance of the resulting clustering. The algorithm stops when there is no further change.

### Clustering

```{r}
c1<-pam(pol_ds_scaled, chosen_size)
pol_ds$cluster_pam <- c1$clustering
```

After assigning we can easily show counts per cluster. 

```{r}
pol_ds %>% group_by(cluster_pam) %>% count()
```

### Evaluation indicators

## CLARA

### Definition of CLARA

CLARA draws multiple samples of the dataset and then applies PAM on each sample. Therefore the main weakness of the PAM procedure is omitted.

### Clustering

```{r}
clara_flex <- eclust(pol_ds_scaled %>% as.matrix(), "clara", k=chosen_size) 
pol_ds$cluster_clara <- clara_flex$clustering
```

After assigning we can easily show counts per cluster. 

```{r}
pol_ds %>% group_by(cluster_clara) %>% count()
```

### Evaluation indicators

```{r}
fviz_cluster(clara_flex, geom="point", ellipse.type="norm") 
```

# Hierarchical clustering

Main advantage of hierarchical clustering over k-means is that there is no need
to implicitly specify number of clusters. The algorithm itself proceeds iteratively,
until there is just a single cluster. The algorithm is very often visualized
via dendogram which is a tree structure. Two main types of hierarchical 
clustering are widely discussed which are agglomerative [bottom up] 
and divisive [top down]. These algorithms work well with arbitrary shape and 
types. The algorithms are often able to detect hierarchical relationships which 
can be useful in many disciplines such as biology or economics. Unfortunately,
these types of algorithms do not work efficiently with large data sets.

## Agglomerative Nesting

```{r}
hc3 <- agnes(pol_ds_scaled, method = "ward")
pltree(hc3, cex = 0.6, hang = -1, main = "dendrogram - agnes")
```

In order to perform hierarchical clustering euclidean distance is calculated first
then we cut the dendogram tree at k = 6. The chosen method is Ward's method 
which is usually fine by default.

```{r}
d_pol_ds <- dist(pol_ds_scaled, method = "euclidean")
hc5 <- hclust(d_pol_ds, method = "ward.D2" )
sub_grp <- cutree(hc5, k = chosen_size)

pol_ds <- 
  pol_ds %>%
  mutate(cluster_hier_ward_D2 = sub_grp) 

pol_ds %>%
  group_by(cluster_hier_ward_D2) %>% 
  count()
```


```{r}
fviz_cluster(list(data = pol_ds_scaled, cluster = pol_ds$cluster_hier_ward_D2))
```

### Evaluation indicators

# Some of other clustering methods

## Divisive clustering

```{r}
hc4 <- diana(pol_ds_scaled)
pltree(hc4, cex = 0.6, hang = -1, main = "dendrogram - diana")
sub_grp <- cutree(hc4, k = chosen_size)

pol_ds <- 
  pol_ds %>%
  mutate(cluster_hier_diana = sub_grp) 

pol_ds %>%
  group_by(cluster_hier_diana) %>% 
  count()
```

In order to perform hierarchical clustering euclidean distance is calculated first
then we cut the dendogram tree at k = 6. The chosen method is Ward's method 
which is usually fine by default. 

```{r}
d_pol_ds <- dist(pol_ds_scaled, method = "euclidean")
hc5 <- hclust(d_pol_ds, method = "ward.D2" )
sub_grp <- cutree(hc5, k = chosen_size)

pol_ds <- 
  pol_ds %>%
  mutate(cluster_hier_wardD2 = sub_grp) 

pol_ds %>%
  group_by(cluster_hier_wardD2) %>% 
  count()
```


```{r}
fviz_cluster(list(data = pol_ds_scaled, cluster = pol_ds$cluster_hier_wardD2))
```

### Evaluation indicators

```{r}
intern <- clValid(pol_ds_scaled %>% as.matrix(), 
                  chosen_size, 
                  clMethods = 
                    c("hierarchical", "kmeans", "diana", "agnes", "pam",
                      "clara", "model"), 
                  validation = "internal"
                  )
summary(intern)
```

The highest connectivity is associated with PAM, CLARA and model clustering 
algorithms. It should be minimized so taking into account this technique 
the best algorithm is diana. Silhouette value is smallest for model based 
and the maximum for diana algorithm. Based on this data we can assume that 
based clustering winner algorithm is diana. 

## Choosing appropriate names for clusters

Based on per cluster results we can try to name formed clusters based on these 
values in terms of international economy

```{r}
pol_ds_long <- 
  pol_ds %>% 
  select(-year, -cluster_kmeans, -cluster_pam,
  -cluster_clara, -cluster_hier_ward_D2, -cluster_hier_wardD2) %>% 
  select(starts_with("v2x_"), cluster_hier_diana) %>% 
  pivot_longer(cols = c(-cluster_hier_diana))
 
pol_ds_long %>% 
  ggplot(aes(x = factor(cluster_hier_diana), 
             y = value, 
             fill= factor(cluster_hier_diana))) +
  geom_boxplot() +
  facet_wrap(~name, ncol = 5, shrink = FALSE, scales = "free") +
  labs(title = 'Boxplots per variable per cluster') +
  xlab("Cluster") + 
  ylab("Value") +
  theme(legend.position = "none")
```

```{r}
pol_ds_long <-
    pol_ds %>% 
    select(-year, -cluster_kmeans, -cluster_pam,
           -cluster_clara, -cluster_hier_ward_D2, -cluster_hier_wardD2) %>% 
    select(-contains("v2"), cluster_hier_diana) %>%
    pivot_longer(cols = c(-cluster_hier_diana))
 
pol_ds_long %>% 
  ggplot(aes(x = factor(cluster_hier_diana), 
             y = value, 
             fill= factor(cluster_hier_diana))) +
  geom_boxplot() +
  facet_wrap(~name, ncol = 5, shrink = FALSE, scales = "free") +
  labs(title = 'Boxplots per variable per cluster') +
  xlab("Cluster") + 
  ylab("Value") +
  theme(legend.position = "none")
```

# Some other clustering methods

## Gaussian mixture models

The main  idea that is hidden in this algorithm is that points belong to several
Gaussian distributions. The main advantages is that there is a proper mathematical
theory developed inside which can lead to very realistic probability of belonging.
Considering drawbacks we cannot ommit is that many parameters need to be specified
for these algorithms.

## Density based algorithms

The main idea behind these kind of algorithms is that cluster is assigned based on points density distribution of data points. DBSCAN algorithm is the most widely discussed algorithm that takes into account two parameters. The first one is the radius of the neighborhood. The second one is is the minimum number of points that belongs to the neighborhood.

## Model Based Clustering

The most important part of this algorithms is that for each cluster different, the best model is fitted. Two main kinds of model-based clustering algorithms can be distinguished which are based on statistical learning method (COBWEB, GMM) and the other based on neural network learning method (SOM and ART).

# Dimensionality Reduction

The idea that is behind dimensionality reduction is to present dataset 
by fewer dimensions in order to make it more interpretable, easier to visualize,
less computationally heavy. Having a dataset with a lot of variables they 
might be correlated with each other and a variable can be presented as 
a combination of other features. What is important new features should not
be correlated between themselves! 

The most common approach for reducing number of features is the following:
1. Decide on data standardization. 
2. Compute covariance matrix with all possible pairs.
3. Based on point 2 result compute eigenvectors and eigenvalues for PCA.
4. Find top eigenvectors and eigenvalues.
5. New principal components are variables as constraint of the variables with highest variance.
6. Develop new features with columns as eigenvectors.
7. Recast data along new principal axes.

```{r}
pacman::p_load(corrplot, maptools, smacof, gridExtra, psych)
```

## Checking correlations
```{r}
pol_ds_scaled_cor <- cor(pol_ds_scaled, method="pearson")
corrplot(pol_ds_scaled_cor, order ="alphabet", tl.cex=0.6)
```

Although there a lot of variables on the plot we can clearly see that a lot of variables are correlated either negatively or positively. 

# MDS reduction

As our data relates to social sciences we can definitely start with multidimensional scaling. Starting with definition, this is a nonlinear statistical technique originated in psychometrics. MDS keeps the distance between the points and reduces number of dimensions. Distances allow to preserve pattern and clusters. The metrics used in this methods, called proximity, describes there is a number that indicates how similar two objects are or are perceived to be. 

## Classification of multidimensional scaling models

In order to describe scaling models they can be broadly classified into metric vs non-metric and strain (classical scaling) vs stress (distance scaling) based MDS models. 

## Applications 

As MDS is one of the most widely used MDS techniques in data analysis of course it is not possible to provide a complete set of its uses. They find several applications in image processing or pre-process for algorithms that rely on Euclidean distances.  

```{r}
dist_pol_ds_scaled<-dist(t(pol_ds_scaled)) # as input we need distance between units
mds3<-cmdscale(dist_pol_ds_scaled, k=2) #k - the maximum dimension of the space
plot(mds3, type='n') # plot with labels
pointLabel(mds3, labels = pol_ds %>% mutate(year = as.character(year)) %>% pull(year), cex=0.6, adj=0.5)
```

Using smacof package allows for different measurement scales of data such as
ratio, interval, oridinal and mspline. 

```{r}
dis2<-sim2diss(pol_ds_scaled_cor, method=1, to.dist=TRUE)
fit.data<-mds(dis2, ndim=2,  type="ratio") # from smacof::
fit.data
summary(fit.data)
plot(fit.data)
```

We can also plot PCA with impact per observation for stress function. 

```{r}
plot(fit.data, pch=21, cex=as.numeric(fit.data$spp), bg="red")
```


### Measuring goodness of MDS 

In order to measure how good is MDS functionality stress function can be 
taken into account. 

```{r}
fit.data$stress
```

Proposes values have been proposed for measuring the goodness of the fit. 0.1 which
is our case provides information that this is a fair result. This value should be 
minimized. For this different methods can be analyzed.

```{r}
dis2<-sim2diss(pol_ds_scaled_cor, method=1, to.dist=TRUE)
fit.data<-mds(dis2, ndim=2,  type="ordinal") # from smacof::
fit.data$stress
```

Smallest value is yield by ordinal method which is non-metric MDS type where only thing that is important is the order relations between the dissimilarities.

## Principal Component Analysis

```{r}
xxx.pca1<-prcomp(pol_ds_scaled, center=FALSE, scale.=FALSE) # stats::
xxx.pca1
xxx.pca1$rotation
summary(xxx.pca1)
```

For explaining 99% of the variance we need to take only first two Principal
Components.

```{r}
xxx.pca2<-princomp(pol_ds_scaled) # stats::princomp()
loadings(xxx.pca2)
plot(xxx.pca2)# the same will be plot(xxx.pca1)
fviz_pca_var(xxx.pca1, col.var="steelblue")
```

Based on this plot the following intuition can be achieved: 
parreg_P4 is oppositely correlated to the v2x_divparctrl_VDEM.

```{r}
fviz_eig(xxx.pca1, choice='eigenvalue') # eigenvalues on y-axis
fviz_eig(xxx.pca1) # percentage of explained variance on y-axis
```

```{r}
var<-get_pca_var(xxx.pca1)
a<-fviz_contrib(xxx.pca1, "var", axes=1, xtickslab.rt=90) # default angle=45Â°
b<-fviz_contrib(xxx.pca1, "var", axes=2, xtickslab.rt=90)
grid.arrange(a,b,top='Contribution to the first two Principal Components')
```

Rotated PCA allows easier interpretation of factors used in analysis due to the change in structure of components. PCA rotates components across the axis of the factors. The most commonly used option is varimax which minimizes the number of variables needed to explain a given factors. 

```{r}
xxx.pca4<-principal(pol_ds_scaled, nfactors=2, rotate="varimax")
xxx.pca4
```

```{r}
summary(xxx.pca4)
```

```{r}
# printing only the significant loadings
print(loadings(xxx.pca4), digits=3, cutoff=0.4, sort=TRUE)
```

```{r}
plot(xxx.pca1)
plot(xxx.pca1, type = "l")
fviz_eig(xxx.pca1)
```

## Uniqueness and complexity

Uniqueness is the relation of variance according to share in other variables. For PCA we want it to be low, then we know that the variable is not correlated with other variables in the model. This is similar to multicollinearity. 

Complexity corresponds to the number of factor loads that take values greater than zero. Given the relatively large loading of one factor only the complexity is near to 1. To paraphrase, we want to know what is the overall contribution of all factors to the single variable. We want to keep our contribution low because it involves a more difficult interpretation of factors.

```{r}
set<-data.frame(complex=xxx.pca4$complexity, unique=xxx.pca4$uniqueness)
set.worst<-set[set$complex>1.8 & set$unique>0.78,]
set.worst
```

Combining complexity and uniqueness together we can spot variables that may lead to problems. 

```{r}
fviz_pca_ind(xxx.pca1, col.ind="cos2", geom="point", gradient.cols=c("white", "#2E9FDF", "#FC4E07" ))
```

```{r}
fviz_pca_var(xxx.pca1, col.var = "steelblue")
```


